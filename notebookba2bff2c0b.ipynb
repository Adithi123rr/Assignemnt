{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras \n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport json\nfrom time import time\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM\n#from tensorflow.keras.layers.Merge import add\nfrom tensorflow.python.keras.layers.merge import add\nimport seaborn as sns\nimport glob\nimport cv2\nfrom tensorflow.keras.preprocessing.image import load_img,img_to_array\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:08:43.356740Z","iopub.execute_input":"2021-09-22T12:08:43.357006Z","iopub.status.idle":"2021-09-22T12:08:43.364899Z","shell.execute_reply.started":"2021-09-22T12:08:43.356977Z","shell.execute_reply":"2021-09-22T12:08:43.363991Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"#### Setting path for image and Pickle file\n","metadata":{}},{"cell_type":"code","source":"Pickle_file_path='/kaggle/input/flickerimage-dataset/'\nimage_dataset_path='/kaggle/input/flickerimage-dataset/Image_captioning_Dataset/Flicker8k_Dataset/'","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:47:21.973179Z","iopub.execute_input":"2021-09-22T11:47:21.973668Z","iopub.status.idle":"2021-09-22T11:47:21.977585Z","shell.execute_reply.started":"2021-09-22T11:47:21.973630Z","shell.execute_reply":"2021-09-22T11:47:21.976647Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Reading pickle file to load caption data  and some analysis on the same","metadata":{}},{"cell_type":"code","source":"pickle_read = pd.read_pickle(Pickle_file_path + 'set_0.pkl')\ncaptions = [item.replace(\"\\t\", \"     \") for item in pickle_read]","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:47:28.991420Z","iopub.execute_input":"2021-09-22T11:47:28.992343Z","iopub.status.idle":"2021-09-22T11:47:29.065819Z","shell.execute_reply.started":"2021-09-22T11:47:28.992272Z","shell.execute_reply":"2021-09-22T11:47:29.064853Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Lets check data in caption List for Overview\ncaptions[0:5]","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:52:10.910972Z","iopub.execute_input":"2021-09-22T11:52:10.911251Z","iopub.status.idle":"2021-09-22T11:52:10.921137Z","shell.execute_reply.started":"2021-09-22T11:52:10.911221Z","shell.execute_reply":"2021-09-22T11:52:10.920400Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"len(captions)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:52:19.050265Z","iopub.execute_input":"2021-09-22T11:52:19.050973Z","iopub.status.idle":"2021-09-22T11:52:19.055751Z","shell.execute_reply.started":"2021-09-22T11:52:19.050939Z","shell.execute_reply":"2021-09-22T11:52:19.055071Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# creating a \"descriptions\" dictionary  where key is 'img_name' and value is list of captions corresponding to that image_file.\n\ndescriptions = {}\n\nfor ele in captions:\n    i_to_c = ele.split(\"\\t\")\n    img_name = i_to_c[0].split(\".\")[0]\n    cap = i_to_c[0]\n    \n    if descriptions.get(img_name) == None:\n        descriptions[img_name] = []\n\n    descriptions[img_name].append(cap)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:52:25.632037Z","iopub.execute_input":"2021-09-22T11:52:25.632686Z","iopub.status.idle":"2021-09-22T11:52:25.671687Z","shell.execute_reply.started":"2021-09-22T11:52:25.632647Z","shell.execute_reply":"2021-09-22T11:52:25.670975Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Convert loaded pickel data intpo dictionary \ndef load_caption_file(path):\n    \n    # dictionary to store captions\n    captions_dict = {}\n    \n    # iterate through the file\n    for caption in captions:\n    \n        # caption has format-> 1000268201_693b08cb0e.jpg#0  A child in a pink dress is climbing up a set of stairs in an entry way .\n        tokens = caption.split()\n        caption_id, caption_text = tokens[0].split('.')[0], tokens[1:]\n        caption_text = ' '.join(caption_text)\n        \n        # save it in the captions dictionary\n        if caption_id not in captions_dict:\n            captions_dict[caption_id] = caption_text\n        \n    return captions_dict\n\n# call the function\ncaptions_dict = load_caption_file(captions)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:52:30.906593Z","iopub.execute_input":"2021-09-22T11:52:30.907276Z","iopub.status.idle":"2021-09-22T11:52:30.960260Z","shell.execute_reply.started":"2021-09-22T11:52:30.907237Z","shell.execute_reply":"2021-09-22T11:52:30.959607Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"descriptions['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:52:49.137765Z","iopub.execute_input":"2021-09-22T11:52:49.138042Z","iopub.status.idle":"2021-09-22T11:52:49.145735Z","shell.execute_reply.started":"2021-09-22T11:52:49.138014Z","shell.execute_reply":"2021-09-22T11:52:49.144853Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Importing image data ","metadata":{}},{"cell_type":"code","source":"import os\nos.chdir(image_dataset_path)\nimages=glob.glob(\"*.jpg\")","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:52:55.927267Z","iopub.execute_input":"2021-09-22T11:52:55.927849Z","iopub.status.idle":"2021-09-22T11:52:56.245213Z","shell.execute_reply.started":"2021-09-22T11:52:55.927813Z","shell.execute_reply":"2021-09-22T11:52:56.244486Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import os \njpgs = os.listdir(image_dataset_path)\nprint(\"The number of jpg flies in Flicker30k: {}\".format(len(jpgs)))","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:52:58.467273Z","iopub.execute_input":"2021-09-22T11:52:58.467829Z","iopub.status.idle":"2021-09-22T11:52:58.477002Z","shell.execute_reply.started":"2021-09-22T11:52:58.467795Z","shell.execute_reply":"2021-09-22T11:52:58.476256Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Plot at least two samples and their captions","metadata":{}},{"cell_type":"code","source":"##\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(20,20), subplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\n    img = cv2.imread(images[i])\n    name=images[i]\n    retrieved_elements = list(filter(lambda x: name in x, captions))\n    new = str([i.split('   ')[1] for i in retrieved_elements] )[1:-1]\n    img = cv2.resize(img, (500,500))\n    ax.imshow(img)\n    ax.set_title(new)\n    \n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:53:06.171416Z","iopub.execute_input":"2021-09-22T11:53:06.171676Z","iopub.status.idle":"2021-09-22T11:53:06.683820Z","shell.execute_reply.started":"2021-09-22T11:53:06.171648Z","shell.execute_reply":"2021-09-22T11:53:06.682990Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# clean the captions\nimport string\n\n# dictionary to store the cleaned captions\nnew_captions_dict = {}\n\n# prepare translation table for removing punctuation. third argument is the list of punctuations we want to remove\ntable = str.maketrans('', '', string.punctuation)\n\n# loop through the dictionary\nfor caption_id, caption_text in captions_dict.items():\n    # tokenize the caption_text\n    caption_text = caption_text.split()\n    # convert it into lower case\n    caption_text = [token.lower() for token in caption_text]\n    # remove punctuation from each token\n    caption_text = [token.translate(table) for token in caption_text]\n    # remove all the single letter tokens like 'a', 's'\n    caption_text = [token for token in caption_text if len(token)>1]\n    # store the cleaned captions\n    new_captions_dict[caption_id] = 'startseq ' + ' '.join(caption_text) + ' endseq'","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:53:22.915000Z","iopub.execute_input":"2021-09-22T11:53:22.915569Z","iopub.status.idle":"2021-09-22T11:53:22.999033Z","shell.execute_reply.started":"2021-09-22T11:53:22.915530Z","shell.execute_reply":"2021-09-22T11:53:22.998357Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"image_index = list(descriptions.keys())\ncaption_images_list = [ image.split('.')[0] for image in os.listdir()if image.split('.')[0] in image_index ]","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:53:29.467017Z","iopub.execute_input":"2021-09-22T11:53:29.467275Z","iopub.status.idle":"2021-09-22T11:53:30.197039Z","shell.execute_reply.started":"2021-09-22T11:53:29.467247Z","shell.execute_reply":"2021-09-22T11:53:30.196305Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"len(caption_images_list)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:53:32.676690Z","iopub.execute_input":"2021-09-22T11:53:32.677367Z","iopub.status.idle":"2021-09-22T11:53:32.683758Z","shell.execute_reply.started":"2021-09-22T11:53:32.677307Z","shell.execute_reply":"2021-09-22T11:53:32.681591Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_images = caption_images_list[0:8025]  ","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:53:37.993038Z","iopub.execute_input":"2021-09-22T11:53:37.993810Z","iopub.status.idle":"2021-09-22T11:53:37.998222Z","shell.execute_reply.started":"2021-09-22T11:53:37.993759Z","shell.execute_reply":"2021-09-22T11:53:37.997489Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"test_images = caption_images_list[8026:8036]","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:53:41.266640Z","iopub.execute_input":"2021-09-22T11:53:41.267188Z","iopub.status.idle":"2021-09-22T11:53:41.271159Z","shell.execute_reply.started":"2021-09-22T11:53:41.267150Z","shell.execute_reply":"2021-09-22T11:53:41.270395Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"len(test_images)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:53:46.228686Z","iopub.execute_input":"2021-09-22T11:53:46.228954Z","iopub.status.idle":"2021-09-22T11:53:46.234281Z","shell.execute_reply.started":"2021-09-22T11:53:46.228925Z","shell.execute_reply":"2021-09-22T11:53:46.233631Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"len(train_images)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:53:48.626765Z","iopub.execute_input":"2021-09-22T11:53:48.627304Z","iopub.status.idle":"2021-09-22T11:53:48.632083Z","shell.execute_reply.started":"2021-09-22T11:53:48.627263Z","shell.execute_reply":"2021-09-22T11:53:48.631431Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# extract features from each photo in the directory\ndef extract_feat(filename):\n    # load the model\n    model = VGG16()\n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    # load the photo\n    image = load_img(filename, target_size=(224, 224))\n    # convert the image pixels to a numpy array\n    image = img_to_array(image)\n    # reshape data for the model\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    # prepare the image for the VGG model\n    image = preprocess_input(image)\n    # get features\n    feature = model.predict(image, verbose=0)\n    return feature\n\n# map an integer to a word\ndef word_for_id(integer, tokenizr):\n    for word, index in tokenizr.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:53:54.925800Z","iopub.execute_input":"2021-09-22T11:53:54.926080Z","iopub.status.idle":"2021-09-22T11:53:54.933167Z","shell.execute_reply.started":"2021-09-22T11:53:54.926049Z","shell.execute_reply":"2021-09-22T11:53:54.932503Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## image feature extractioN using VGG-16","metadata":{}},{"cell_type":"code","source":"##Image Feature Extractor using VGG-16 model\n# extract features from each photo in the directory\ndef extract_features(directory, image_keys):\n    # load the model\n    model = VGG16()\n    \n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    \n    # summarize\n    print(model.summary())\n    \n    # extract features from each photo\n    features = dict()\n    \n    \n    for i,name in enumerate(jpgs):\n        # load an image from file\n        filename = image_dataset_path + '/' + name\n        \n        \n        # load an image from file\n        filename = directory + '/' + name \n        \n        # load the image and convert it into target size of 224*224\n        image = load_img(filename, target_size=(224, 224))\n        \n        # convert the image pixels to a numpy array\n        image = img_to_array(image)\n        \n        # reshape data for the model\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        \n        # prepare the image for the VGG model\n        image = preprocess_input(image)\n        \n        # get features\n        feature = model.predict(image, verbose=0)\n        \n        # get image id\n        image_id = name.split('.')[0]\n        \n        # store feature\n        features[image_id] = feature\n        \n#         print('>%s' % name)\n        \n\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:54:00.164108Z","iopub.execute_input":"2021-09-22T11:54:00.164684Z","iopub.status.idle":"2021-09-22T11:54:00.172792Z","shell.execute_reply.started":"2021-09-22T11:54:00.164642Z","shell.execute_reply":"2021-09-22T11:54:00.171893Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_validate_features = extract_features(image_dataset_path, train_images)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T11:54:08.472819Z","iopub.execute_input":"2021-09-22T11:54:08.473082Z","iopub.status.idle":"2021-09-22T12:00:48.332142Z","shell.execute_reply.started":"2021-09-22T11:54:08.473052Z","shell.execute_reply":"2021-09-22T12:00:48.331386Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(\"{} : {}\".format(list(train_validate_features.keys())[0], train_validate_features[list(train_validate_features.keys())[0]] ))","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:02:36.909673Z","iopub.execute_input":"2021-09-22T12:02:36.909999Z","iopub.status.idle":"2021-09-22T12:02:36.918735Z","shell.execute_reply.started":"2021-09-22T12:02:36.909944Z","shell.execute_reply":"2021-09-22T12:02:36.917739Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from pickle import dump\ndump(train_validate_features, open('/kaggle/working/train_validate_features.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:02:39.115392Z","iopub.execute_input":"2021-09-22T12:02:39.116080Z","iopub.status.idle":"2021-09-22T12:02:39.429575Z","shell.execute_reply.started":"2021-09-22T12:02:39.116042Z","shell.execute_reply":"2021-09-22T12:02:39.428769Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# make a dictionary of image with caption for train_validate_images\ntrain_validate_image_caption = {}\n\nfor image, caption in new_captions_dict.items():\n    \n    # check whether the image is available in both train_validate_images list and train_validate_features dictionary\n    if image in train_images and image in list(train_validate_features.keys()):\n        \n         train_validate_image_caption.update({image : caption})\n\nlen(train_validate_image_caption)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:02:43.132772Z","iopub.execute_input":"2021-09-22T12:02:43.133031Z","iopub.status.idle":"2021-09-22T12:02:45.472937Z","shell.execute_reply.started":"2021-09-22T12:02:43.133002Z","shell.execute_reply":"2021-09-22T12:02:45.471951Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"list(train_validate_image_caption.values())[1]","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:02:55.094435Z","iopub.execute_input":"2021-09-22T12:02:55.094982Z","iopub.status.idle":"2021-09-22T12:02:55.100198Z","shell.execute_reply.started":"2021-09-22T12:02:55.094945Z","shell.execute_reply":"2021-09-22T12:02:55.099248Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nImage(image_dataset_path+'/'+list(train_validate_image_caption.keys())[1]+'.jpg')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:03:02.826066Z","iopub.execute_input":"2021-09-22T12:03:02.826775Z","iopub.status.idle":"2021-09-22T12:03:02.844277Z","shell.execute_reply.started":"2021-09-22T12:03:02.826736Z","shell.execute_reply":"2021-09-22T12:03:02.843404Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# load libraries\nimport numpy as np\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical, plot_model\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:03:08.157283Z","iopub.execute_input":"2021-09-22T12:03:08.157575Z","iopub.status.idle":"2021-09-22T12:03:08.162408Z","shell.execute_reply.started":"2021-09-22T12:03:08.157547Z","shell.execute_reply":"2021-09-22T12:03:08.161708Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# initialise tokenizer\ntokenizer = Tokenizer()\n\n# create word count dictionary on the captions list\ntokenizer.fit_on_texts(list(train_validate_image_caption.values()))\n\n# how many words are there in the vocabulary? store the total length in vocab_len and add 1 because word_index starts with 1 not 0 \nvocab_len = len(tokenizer.word_index) + 1\n\n# store the length of the maximum sentence\nmax_len = max(len(train_validate_image_caption[image].split()) for image in train_validate_image_caption)\n\ndef prepare_data(image_keys):\n    \n    # x1 will store the image feature, x2 will store one sequence and y will store the next sequence\n    x1, x2, y = [], [], []\n\n    # iterate through all the images \n    for image in image_keys:\n\n        # store the caption of that image\n        caption = train_validate_image_caption[image]\n\n        # split the image into tokens\n        caption = caption.split()\n\n        # generate integer sequences of the\n        seq = tokenizer.texts_to_sequences([caption])[0]\n\n        length = len(seq)\n\n        for i in range(1, length):\n\n            x2_seq, y_seq = seq[:i] , seq[i]  \n\n            # pad the sequences\n            x2_seq = pad_sequences([x2_seq], maxlen = max_len)[0]\n\n\n            # encode the output sequence                \n            y_seq = to_categorical([y_seq], num_classes = vocab_len)[0]\n\n            x1.append( train_validate_features[image][0] )\n\n            x2.append(x2_seq)\n\n            y.append(y_seq)\n               \n    return np.array(x1), np.array(x2), np.array(y)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:03:12.958720Z","iopub.execute_input":"2021-09-22T12:03:12.959255Z","iopub.status.idle":"2021-09-22T12:03:13.096381Z","shell.execute_reply.started":"2021-09-22T12:03:12.959217Z","shell.execute_reply":"2021-09-22T12:03:13.095643Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Preparing Train data,Test and Validation Data for Model\n","metadata":{}},{"cell_type":"code","source":"train_x1, train_x2, train_y = prepare_data( train_images[0:7081] )\nvalidate_x1, validate_x2, validate_y = prepare_data( train_images[7081:8081] )","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:03:18.932029Z","iopub.execute_input":"2021-09-22T12:03:18.932282Z","iopub.status.idle":"2021-09-22T12:03:23.208765Z","shell.execute_reply.started":"2021-09-22T12:03:18.932254Z","shell.execute_reply":"2021-09-22T12:03:23.207863Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"len(train_x1)","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:03:24.601847Z","iopub.execute_input":"2021-09-22T12:03:24.602704Z","iopub.status.idle":"2021-09-22T12:03:24.612211Z","shell.execute_reply.started":"2021-09-22T12:03:24.602665Z","shell.execute_reply":"2021-09-22T12:03:24.611376Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Creating model, Printing Summary  And model compilation","metadata":{}},{"cell_type":"code","source":"# feature extractor model\ninput_1 = Input(shape=(4096,))\ndroplayer = Dropout(0.5)(input_1)\ndenselayer = Dense(256, activation='relu')(droplayer)\n\n# sequence model\ninput_2 = Input(shape=(max_len,))\nembedding = Embedding(vocab_len, 256, mask_zero=True)(input_2)\ndroplayer_ = Dropout(0.5)(embedding)\nlstm = LSTM(256)(droplayer_)\nlstm2 = LSTM(256)(droplayer_)\nlstm3 = LSTM(256)(droplayer_)\n\n# decoder model\ndecoder1 = add([denselayer, lstm,lstm2,lstm3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_len, activation='softmax')(decoder2)\n\n# tie it together [image, seq] [word]\nmodel = Model(inputs=[input_1, input_2], outputs=outputs)\nmodel.compile(loss='categorical_crossentropy', optimizer = tf.optimizers.Adam(learning_rate=0.0001))\n\n# summarize model\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:08:55.058841Z","iopub.execute_input":"2021-09-22T12:08:55.059106Z","iopub.status.idle":"2021-09-22T12:08:57.315489Z","shell.execute_reply.started":"2021-09-22T12:08:55.059077Z","shell.execute_reply":"2021-09-22T12:08:57.314801Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"# fit model\nhistory = model.fit([train_x1, train_x2],  \n                    train_y,              \n                    verbose = 1,            \n                    epochs = 5,\n                    batch_size=64,\n                    validation_data=([validate_x1, validate_x2], validate_y)) ","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:09:07.701192Z","iopub.execute_input":"2021-09-22T12:09:07.701914Z","iopub.status.idle":"2021-09-22T12:29:14.884727Z","shell.execute_reply.started":"2021-09-22T12:09:07.701876Z","shell.execute_reply":"2021-09-22T12:29:14.883852Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### plot training loss and validation loss","metadata":{}},{"cell_type":"code","source":"# plot training loss and validation loss\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:31:00.278346Z","iopub.execute_input":"2021-09-22T12:31:00.278880Z","iopub.status.idle":"2021-09-22T12:31:00.524026Z","shell.execute_reply.started":"2021-09-22T12:31:00.278843Z","shell.execute_reply":"2021-09-22T12:31:00.523383Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# saving the model with last parameter \nmodel.save('/kaggle/working/model.h5')","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:31:17.771979Z","iopub.execute_input":"2021-09-22T12:31:17.772248Z","iopub.status.idle":"2021-09-22T12:31:17.955347Z","shell.execute_reply.started":"2021-09-22T12:31:17.772213Z","shell.execute_reply":"2021-09-22T12:31:17.954354Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# extract features from each photo in the directory\ndef extract_feat(filename):\n    # load the model\n    model = VGG16()\n    # re-structure the model\n    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n    # load the photo\n    image = load_img(filename, target_size=(224, 224))\n    # convert the image pixels to a numpy array\n    image = img_to_array(image)\n    # reshape data for the model\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    # prepare the image for the VGG model\n    image = preprocess_input(image)\n    # get features\n    feature = model.predict(image, verbose=0)\n    return feature\n\n# map an integer to a word\ndef word_for_id(integer, tokenizr):\n    for word, index in tokenizr.word_index.items():\n        if index == integer:\n            return word\n    return None\n ","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:34:22.281574Z","iopub.execute_input":"2021-09-22T12:34:22.281847Z","iopub.status.idle":"2021-09-22T12:34:22.288939Z","shell.execute_reply.started":"2021-09-22T12:34:22.281819Z","shell.execute_reply":"2021-09-22T12:34:22.287679Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n    # seed the generation process\n    in_text = 'startseq'\n    # iterate over the whole length of the sequence\n    for i in range(max_length):\n        # integer encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        # predict next word\n        yhat = model.predict([photo,sequence], verbose=0)\n        # convert probability to integer\n        yhat = np.argmax(yhat)\n        # map integer to word\n        word = word_for_id(yhat, tokenizer)\n        # stop if we cannot map the word\n        if word is None:\n            break\n        # append as input for generating the next word\n        in_text += ' ' + word\n        # stop if we predict the end of the sequence\n        if word == 'endseq':\n            break\n    return in_text","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:34:52.431662Z","iopub.execute_input":"2021-09-22T12:34:52.431938Z","iopub.status.idle":"2021-09-22T12:34:52.440203Z","shell.execute_reply.started":"2021-09-22T12:34:52.431900Z","shell.execute_reply":"2021-09-22T12:34:52.439288Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n# load the model\nmodl = load_model('/kaggle/working/model.h5')\n\n# generate description\ntokenizr = Tokenizer()\ntokenizr.fit_on_texts([caption for image, caption in new_captions_dict.items() if image in train_images])\nmax_length = 31\n\nfor count in range(10):\n\n    photo = extract_feat('{}.jpg'.format(image_dataset_path+'/'+train_images[count]))  \n\n    # generate description\n    description = generate_desc(modl, tokenizr, photo, max_length)\n    print(\"Predicted caption -> \", description)\n    print()\n    print(\"Actual caption -> \", new_captions_dict[train_images[count]])\n    print('*********************************************************************')\n    print()","metadata":{"execution":{"iopub.status.busy":"2021-09-22T12:39:05.265099Z","iopub.execute_input":"2021-09-22T12:39:05.265404Z","iopub.status.idle":"2021-09-22T12:39:35.370903Z","shell.execute_reply.started":"2021-09-22T12:39:05.265370Z","shell.execute_reply":"2021-09-22T12:39:35.370148Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}